* ideas

We can use fundamental types of Constructors (+,Disjoint union, inductive types)
and Record Types (Product, *, tables)
to describe the first layer of the asts as an algebra.

translate these layers of ppx ocaml into topological math theory 
** layers
*** Driver transformation 1
**** List of structure_item (record *) 2
***** pstr_desc structure_item_desc (inductive type +) 3
******* Pstr_value (rec_flag, value_binding_list) -> function
******* Pstr_type (rec_flag, type_declaration_list) -> types 4
******** Emitthecode.emit_type_decl_list type_declaration (ptype_name, 5
********* ptype_kind Ptype_record a ->     6
********** record_kind_field pld_type(* : core_type *); 7

********** core_type  8
********** core_type_desc Ptyp_constr "name of type" 9
******* Pstr_module  module_binding -> modules
******* Pstr_open open_description -> includes

** construct a new copy of the program in parts
*** header with includes
**** Open all needed fundamental types
***** Should come from existing opens collected in source module
***** Recursive processing of modules


*** for each type mentioned in the constructor
**** Generate a call to process the fundamental type.
The constructors are called in order

**** Generate a declaration of the stub function to process the fundamental type
this allows for customization of processing.

*** for each type declaration
**** Process constructors with tuples of primitive types as args
***** Process lists and optionals still not working.

**** Process record types with list of fields.
for each field we process its type with another type specific function for each type

* Issues

** higher order types
list, option and loc are higher order types that we currently have problems with.

63 |  let process_type_decl_loc (x:loc):string = match x with {txt(* var_name*);loc(* location*)} ->(process_type_decl_var_name txt)^(process_type_decl_location loc) 
                                   ^^^
Error: The type constructor loc expects 1 argument(s),

open_infos
include_infos

** names with -
one type has the name with a - in it, need to strip it.
"var-name"

*** question
uniting unimath and llm and ppx

runner: llm, used for vectorizing all the terms, and structured subgraph chunks of the data.
complete chunks, reduced in size chunks. ask llm to break larger text into chunks.
the vectorized form is fed back into unimath as input.

blazedb : vector db or graph db of all the data.

original text here maps to chunk a1 here via function f.
chunk a1 has vector A 1 value in unimath.

find paths in graph. deep graph learning model on the unimath graph.

* diagram1

init -> seed text  -> thinker
thinker -> llama
grammar -> llama : contrain output
llama -> token -> ocaml
ocaml -> coq -> parse
ocaml -> stop : terminate conversation
output of parse -> feed back into thinker.

* next idea

1. introspection is part of the orient/decision making phase.
2. oodo is basic loop.
3. proof enginee feed into decision making step.
4. developing theories/proofs is an activity
5. activity uses proof general, coq, etc.
   we can generate coq directly from ppx using a custom plugin.
   we can use llm to customize each case with prompt models and other fun items.
   we want the ability to tie in the llm to the ppx and call it in chunks via the api.
   the api calls the ollama server that has an embedded ocaml inside it as well. so ocaml on the client and server.
   we will use lwt yojson for the client.
   we can extend the protocol to include more metadata extracted with ocaml.
   we can wrap the llama.cpp with the ocaml wrapper and use that to annotate the metadata,
   we can take the ocaml structure of llama and transform it into meta c++, but aslo metacoq/unimath.
   we can take the mistral model and lift into unimath, the computation graph and tensor models and even data relationships we extract.
   all this data can be used by the compiler to produce better code, we can imagine the unimath extraction into c++.
   we can do this by mechanized extraction between ocaml and c++ with some bridge.
   now we can imagine a huge set of templates parameterized with the unimath/metacoq data that we can query at compile or run time.
   then we can manipulate this structure to create new versions of it, mutations
   but we want to constrain them to valid ones, imagine an autoencoder that will only produce valid data.
   the proof engine would constraint and check it further.  
   take unimath and train model on it. be able to predict next code.
   convert types declarations, record structures and constructors from ppx to unimath.
   constrain the grammar with model of the unimath. use coq grammar and reduce it further.
   check the grammar with gbnf/llama.
   replace/rewrite that code with ocaml.
   use the ocaml to parse and return the gbnf structures.
   translate grammar to coq proof, lift into unimath.
   convert metacoq/ppx data into grammar that will match/generate those parts of the data given.
   so translate the metadata into a grammar that can be used by llama.cpp.
   this reduction of strenght is going from level 8 to level 1-3 or so.
   so our theory has to represent systems in various states and be able to process or transform them along.
   we want to translate our data into various forms of the existing unimath structures and show how each concept can be expressed in each other concept or not and why,
   this is a N*N/2 relationship between the types of the system showing how they can represent each other.
   this system can be used behind a llm text prompt as input, so the user would interact with an llm to have it generate instructures for our system.
   the system can be used from dune in compilation calling the llm for items matching the ppx directives.


* so the components are :

** client driver front end LLM with high level goal.
eg prompt : "make me a snake game"
it can also be a complex spec document like this one, larger than a context window.
well the parts then need to be structured with org mode.

It can also be driven by dune ppx-introspector that calls the system for any marked code.

It can be called from an Language Server Protocol LSP(visual studio),
or from proof general in emacs. 

this can also be an "autogpt agent" or some other task that is working with the system.
we can create hybrid systems where our llama.cpp+proof system is used by another driver.
basically we call into ollama.cpp server with embedded ocaml (coq/unimath) using https rest

*** memory architecture
some client might load the model as shared memory as well, but we will just
talk to a server that has it loaded. 
we can imagine all the parts as sharing memory.
need to protect against bad inputs. Also protect users from each other.
need to save/find conversations and contexts in the time repo structure which has directories for each day.

*** queue
we create a queue of jobs to process.
jobs can spawn new jobs.

*** translation from goal (snake game) into unimath model.

we lift the unstructured user input eg "make me a snake game"
into a higher level of formality using a math system for checking and modeling.

this can be done on the inside of the llm, in the forward pass, we can find concepts
relating to our reference knowledge and highlight neighboring concepts automatically,
based on the vectorization of the goal as the specification. it could emit types, records, fields, constructors, template, tables, code
out of the llm model that was highlighted or triggered by the input.
this is the oracle view of the llm model weight internals
or the lexical introspection into the language space.
we can consult the oracle to gain insight into the gods, but it is

alternatively, we can just ask the llm to translate it and contrain the output. 

#+begin_src output
We convert an unstructured user input like "make me a snake game" into
a higher level of formality using a mathematical system for analysis
and modeling. This can be done automatically through a machine
learning model that is trained on a reference knowledge base. As the
model receives the input, it highlights related concepts based on
vectorization of the goal as a specification.

The output of this process could include types, records, fields,
constructors, templates, tables, and code that are triggered or
highlighted in the model. This is often referred to as the "oracle
view" of the model's weight internals or the lexical introspection
into the language space. The oracle provides insight into the concepts
that the input relates to, but it is important to note that it may not
be perfect and should be used with caution.

Alternatively, we can instruct the machine learning model to translate
the input directly and constrain the output to ensure accuracy and
consistency. This approach can be useful for generating specific
outputs that are tailored to a particular use case or application.
#+end_src

***** merge eigenvector
use new prime numbers to re-encode eigenvectors.
how to add weights. minimizes the loss, gradient funtion.
https://github.com/meta-introspector/lang_agent/issues/1


****  iteration over model
traversal of the graph in chunks and using prompt models to generate text.

***** grammar generation
generate grammar from the model to be used.
collect statistics

***** constrained generation

the model generates constrained output using grammar.
this output is parsed by coq and checked for syntax and also correctness.

***** Expanded output
the metacoq data is used to decorate and annotate the output of the llm, syntax highlighting, lsp, semgrep,
snippets from definitions of names referenced,
and annotate the output of the llm, with parse results, test results, proof results, all with urls that contain further links that can be followed or not by the user.
the use also specifies the depth of the data returned,
and rules of inference to follow or not, with weights, like a annotated graph, or weighted graph,
which is a series of numbers that are the weights of a quasi neural network.
we are constructing the values of a neural network peicemeal, or pretraining it or transfering knowledge from existing code in ocaml into the network.

how to merge model weights, or even compose new models out of existing models.
we use eigenvectors as axis to multiply counts of objects of certain types

it is then evaluated back by the llm to create a response guided by the structure of the type system.
this is important for error handling.

#+begin_src output

The Metacoq data serves as a visual enhancement and metadata to the output of an LLM (Language Model). It provides syntax highlighting, LSP (Language Server Protocol) support, SEMGrep functionality, snippets from definition references, and annotates the LLM output with parse results, test results, and proof results. These annotations are linked to URLs that can be followed or not by the user.

The use of Metacoq data allows for fine-tuning of depth and rules of inference. It is a weighted graph representation, which is a series of numbers representing the weights of a quasi-neural network. This enables us to piece together the values of a neural network incrementally, through pretraining or transferring knowledge from existing OCaml code into the network.

Merging and composing new models can be done by using eigenvectors as axes to multiply counts of objects of certain types. This approach is crucial for error handling, ensuring that the LLM response adheres to the structure of the type system.
#+end_src


***** Summary
the results are summarized by multiple passes of the llm.

**** in code

we can write this all in code, in ocaml, or have an llm translate it for us.

*** in spec
we can write this all in our org mode spec and translate it back to ocaml
using pandoc/haskell or native org mode parsers. we found a grammar for org mode.


** oodo
User is driving the system in an OODO loop.

*** observe
behaviour of system, observability and semantic integration,
ppx -introspector, knowledge of types, records and constructors
recursive expansion into levels of detail.

*** orient
understand data in context, llm forward passes inference,
database inference lookups, vectorization,
statistical analysis. 

dfa would count as part of orientation, 

*** decide.
human is needed, if to continue or abort (halting problem)
executive decision.

*** act
take actions in the system. those can be pointers to new urls that resolve to future work.

#+begin_src output

In an OODO loop, the user takes charge of driving the system by
observing its behavior, understanding it in context through forward
passes and inference, making executive decisions, and acting on them.

The first step is observation, where the user introspects the types,
records, and constructors using Ppx. This provides a recursive
expansion into levels of detail that enables the user to understand
the system more deeply.

Next comes orientation, where the user applies LLM forward passes for
inference and database lookup inferences. They also use vectorization
and statistical analysis to gain insights into the data in context.

dfa is a part of orientation as it helps the user count the actions
needed to complete the task at hand. The halting problem comes into
play during decision-making, where the user decides whether to
continue or abort the process based on their own executive
decision-making capabilities.

Finally, the user acts on the system by taking pointers to new URLs
that resolve to future work. This enables them to make decisions and
take action in real-time, making the OODO loop a powerful tool for
driving complex systems forward.
#+end_src

*** User may delegate decisions to others or ai.
